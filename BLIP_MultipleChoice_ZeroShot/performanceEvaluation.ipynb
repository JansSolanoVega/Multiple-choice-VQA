{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading model\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "# Get the current script's directory\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "library_directory = os.path.abspath(os.path.join(current_directory, 'torchVersion', 'BLIP'))\n",
    "sys.path.append(library_directory)\n",
    "from models.blip_vqa import blip_vqa\n",
    "\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
    "#model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_vqa.pth'\n",
    "\n",
    "image_size = 480\n",
    "model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freezing all layers except MLP\n",
    "number_of_last_trainable_layers = 3\n",
    "N_BLIP = 11\n",
    "\n",
    "def containsLastLayers(name):\n",
    "    val = False\n",
    "    for layer in list(range(N_BLIP-number_of_last_trainable_layers,N_BLIP+1)):\n",
    "        if str(layer) in name:\n",
    "            val = True\n",
    "            break\n",
    "    return val\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Keeping Visual encoder MLPs\n",
    "for name, param in model.visual_encoder.named_parameters():\n",
    "    if \"mlp\" in name and containsLastLayers(name):\n",
    "        param.requires_grad = True\n",
    "\n",
    "#Keeping FFNN from Image-grounded text encoder\n",
    "for name, param in model.text_encoder.named_parameters():\n",
    "    if \"attention\" not in name and \"crossattention\" not in name and \"embeddings\" not in name and containsLastLayers(name):\n",
    "        param.requires_grad = True\n",
    "\n",
    "#Keeping FFNN from Answer decoder\n",
    "for name, param in model.text_encoder.named_parameters():\n",
    "    if \"attention\" not in name and \"crossattention\" not in name and \"embeddings\" not in name and containsLastLayers(name):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Adjust the path accordingly\n",
    "from VQA_Dataset_BLIP import VQA_Dataset_preloaded_TorchVersion\n",
    "\n",
    "dataset = VQA_Dataset_preloaded_TorchVersion(device)\n",
    "dataset.compute_store(model.tokenizer, length=15000, fileName=\"embeddingsBLIPWithMultipleChoiceImageSize480Length20_15k_TorchVersion.h5\", real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load(fileName=\"mscoco_embeddingsBLIPWithMultipleChoiceImageSize480Length20_15k_TorchVersion.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test-train split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # 'batch' is a list of samples, each being a dictionary\n",
    "    return batch\n",
    "# def custom_collate_fn(batch):\n",
    "#     # 'batch' is a list of samples, each being a dictionary\n",
    "#     new_batch ={}\n",
    "#     for element in batch:\n",
    "#         imgs_element = element[\"imgs\"]\n",
    "#         questions_input_ids_element = element[\"questions\"][\"input_ids\"]\n",
    "#         questions_attention_element = element[\"questions\"][\"attention_mask\"]\n",
    "\n",
    "#         multiple_answers_input_ids_element = element[\"multiple_answers\"][\"input_ids\"]\n",
    "#         multiple_answers_attention_element = element[\"multiple_answers\"][\"attention_mask\"]\n",
    "\n",
    "#         correct_answers_input_ids_element = element[\"correct_answers\"][\"input_ids\"]\n",
    "#         correct_answers_attention_element = element[\"correct_answers\"][\"attention_mask\"]\n",
    "        \n",
    "        \n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_size = int(len(dataset)*0)\n",
    "val_size = int(len(dataset)*0)\n",
    "test_size = int(len(dataset))-train_size-val_size\n",
    "generator = torch.Generator().manual_seed(1)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)\n",
    "\n",
    "batch_size=2\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "#Checking data\n",
    "fig=plt.figure(0, (28, 15))\n",
    "for num, element in enumerate(test_dataloader):\n",
    "    if num==4:\n",
    "        for i in range(batch_size):\n",
    "            ax = fig.add_subplot(2,int(batch_size/2),i+1)\n",
    "            question_raw = element[i][\"questions\"]\n",
    "            correct_answer_raw = element[i][\"correct_answers\"]\n",
    "            multiple_answers = element[i][\"multiple_answers\"]\n",
    "            question_text = convert_ids_to_string(question_raw[\"input_ids\"].cpu(), model)\n",
    "            correct_answer_text = convert_ids_to_string(correct_answer_raw[\"input_ids\"].cpu(), model)\n",
    "            ax.imshow(np.transpose(element[i][\"imgs\"].squeeze(0).cpu(), (1, 2, 0))) #Converting to matplotlib format\n",
    "            ax.set_title(\"Question: \"+question_text+\"\\nAnswer: \"+correct_answer_text, fontsize=9)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class performanceAnalysis:\n",
    "    def __init__(self):\n",
    "        self.counters = [0, 0, 0]\n",
    "        self.total = [0, 0, 0]\n",
    "    def it(self, correct, answer_type):\n",
    "        if correct:\n",
    "            self.counters[answer_type]+=1\n",
    "        self.total[answer_type]+=1\n",
    "    def get_accuracies(self):\n",
    "        dictionary = {\"yes/no\": self.counters[0]/self.total[0], \"number\": self.counters[1]/self.total[1], \"other\": self.counters[2]/self.total[2], \"total\": sum(self.counters)/sum(self.total)}\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate using generate(w/o considering MC answers as input)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from utils import convert_ids_to_string\n",
    "#Evaluating raw model\n",
    "def eval(dataloader, model):\n",
    "    model.eval()\n",
    "    performAnalysis = performanceAnalysis()\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for element in pbar:\n",
    "            for i in range(batch_size):     \n",
    "                question = element[i][\"questions\"]\n",
    "                img = element[i][\"imgs\"]\n",
    "                correct_answer = element[i][\"correct_answers\"]\n",
    "                #print(convert_ids_to_string(question[\"input_ids\"], model))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred = model(img, question, train=False, inference='generate')    \n",
    "                    predicted_answer = model.tokenizer(pred, padding='max_length', truncation=True, max_length=20,return_tensors=\"pt\").to(device)[\"input_ids\"][0]\n",
    "                    predicted_answer[0] = 30523\n",
    "                    correct_answer = correct_answer[\"input_ids\"][0]\n",
    "                    #print(f\"Predicted: {predicted_answer}, Correct: {correct_answer}\")  \n",
    "                    correct = int(np.array_equal(correct_answer.detach().cpu().numpy(), predicted_answer.detach().cpu().numpy()))\n",
    "\n",
    "                    performAnalysis.it(correct, int(element[i][\"answer_types\"].item()))\n",
    "    return performAnalysis.get_accuracies()\n",
    "eval(test_dataloader, model) #53.125% with 4k images(0.2) (using generate and model_base_vqa_capfilt_large)\n",
    "#54.625% with 4k images(0.2) (using generate and model_base_vqa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate using rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "#Evaluating raw model\n",
    "def eval(dataloader, model):\n",
    "    model.eval()\n",
    "    performAnalysis = performanceAnalysis()\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for element in pbar:\n",
    "            for i in range(batch_size):     \n",
    "                question = element[i][\"questions\"]\n",
    "                img = element[i][\"imgs\"]\n",
    "                multiple_answers = element[i][\"multiple_answers\"]\n",
    "                correct_answer = element[i][\"correct_answers\"]\n",
    "                        \n",
    "                with torch.no_grad():\n",
    "                    idx = model(img, question, answer = multiple_answers, train=False, inference='rank_compressed', k_test=18).item()\n",
    "                    predicted_answer = multiple_answers[\"input_ids\"][idx]\n",
    "                    predicted_answer[0] = 30523\n",
    "                    correct_answer = correct_answer[\"input_ids\"][0]\n",
    "                    #print(f\"Predicted: {predicted_answer}, Correct: {correct_answer}\")  \n",
    "                    correct = int(np.array_equal(correct_answer.detach().cpu().numpy(), predicted_answer.detach().cpu().numpy()))\n",
    "                    performAnalysis.it(correct, int(element[i][\"answer_types\"].item()))\n",
    "    return performAnalysis.get_accuracies()\n",
    "eval(test_dataloader, model) #63.5% with 4k images (using rank and model_base_vqa_capfilt_large)-Abstract\n",
    "#63.875% with 4k images(0.2) (using rank and model_base_vqa_capfilt_large)-Abstract\n",
    "#86.375% with 4k images(0.2) (using rank and model_base_vqa)-Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os \n",
    "\n",
    "#Tensorboard\n",
    "currentModelIteration = \"5e-5_4k\"\n",
    "folder_path = os.path.join(\"runs\", \"trainingsBLIP_TorchVersion\", currentModelIteration)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "writer = SummaryWriter(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, vqa_model, optimizer, clip_value, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    vqa_model.train()\n",
    "    train_cost_acum = 0.0\n",
    "\n",
    "    for batch_i, batch in enumerate(dataloader):    \n",
    "        for i in range(batch_size):     \n",
    "            question = batch[i][\"questions\"]\n",
    "            img = batch[i][\"imgs\"]\n",
    "            correct_answer = batch[i][\"correct_answers\"]\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            loss = model(img, question, n=[1], answer = correct_answer, train=True, weights=torch.tensor([1]).to(device))\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vqa_model.parameters(), max_norm=clip_value, error_if_nonfinite=True)\n",
    "            optimizer.step()\n",
    "            train_cost_acum += loss\n",
    "        \n",
    "        #Ploting results\n",
    "        \n",
    "        if batch_i % 40 == 1:\n",
    "            writer.add_scalar('Loss/training', float(train_cost_acum)/batch_i, epoch * size + batch_i)\n",
    "            #writer.add_scalar('Accuracy/training', float(train_correct_num)/train_total, epoch * size + batch_i)\n",
    "            loss, current = loss.item(), batch_i*batch_size\n",
    "            print(\"loss: \", loss, current, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Batch version\n",
    "# def train(dataloader, vqa_model, optimizer, clip_value, epoch):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     vqa_model.train()\n",
    "#     train_cost_acum = 0.0\n",
    "\n",
    "#     for batch_i, batch in enumerate(dataloader):    \n",
    "          \n",
    "#         question = batch[\"questions\"]\n",
    "#         question[\"input_ids\"] = question[\"input_ids\"].squeeze(1)\n",
    "#         question[\"attention_mask\"] = question[\"attention_mask\"].squeeze(1)\n",
    "\n",
    "#         img = batch[\"imgs\"].squeeze(1)\n",
    "\n",
    "#         correct_answer = batch[\"correct_answers\"]\n",
    "#         correct_answer[\"input_ids\"] = correct_answer[\"input_ids\"].squeeze(1)\n",
    "#         correct_answer[\"attention_mask\"] = correct_answer[\"attention_mask\"].squeeze(1)\n",
    "\n",
    "#         optimizer.zero_grad()  \n",
    "#         loss = model(img, question, n=[1], answer = correct_answer, train=True, weights=torch.tensor([1]).to(device))\n",
    "\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(vqa_model.parameters(), max_norm=clip_value, error_if_nonfinite=True)\n",
    "#         optimizer.step()\n",
    "#         train_cost_acum += loss\n",
    "        \n",
    "#         #Ploting results\n",
    "        \n",
    "#         if batch_i % 40 == 1:\n",
    "#             writer.add_scalar('Loss/training', float(train_cost_acum)/batch_i, epoch * size + batch_i)\n",
    "#             #writer.add_scalar('Accuracy/training', float(train_correct_num)/train_total, epoch * size + batch_i)\n",
    "#             loss, current = loss.item(), batch_i*batch_size\n",
    "#             print(\"loss: \", loss, current, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for element in pbar:\n",
    "            for i in range(batch_size):     \n",
    "                question = element[i][\"questions\"]\n",
    "                img = element[i][\"imgs\"]\n",
    "                multiple_answers = element[i][\"multiple_answers\"]\n",
    "                correct_answer = element[i][\"correct_answers\"]\n",
    "                        \n",
    "                with torch.no_grad():\n",
    "                    idx = model(img, question, answer = multiple_answers, train=False, inference='rank_compressed', k_test=18).item()\n",
    "                    predicted_answer = multiple_answers[\"input_ids\"][idx]\n",
    "                    predicted_answer[0] = 30523\n",
    "                    correct_answer = correct_answer[\"input_ids\"][0]\n",
    "                    #print(f\"Predicted: {predicted_answer}, Correct: {correct_answer}\")  \n",
    "                    correct += int(np.array_equal(correct_answer.detach().cpu().numpy(), predicted_answer.detach().cpu().numpy()))\n",
    "    correct /= size\n",
    "\n",
    "    #Ploting results\n",
    "    writer.add_scalar('Accuracy/test', correct*100, epoch)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%\")\n",
    "    return 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Batch version\n",
    "# def eval(dataloader, model, epoch):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     pbar = tqdm.tqdm(dataloader)\n",
    "#     with torch.no_grad():\n",
    "#         for element in pbar:\n",
    "#             for i in range(batch_size):\n",
    "#                 img = element[\"imgs\"][i]\n",
    "#                 question = {key: value[i] for key, value in element[\"questions\"].items()}\n",
    "#                 multiple_answers = {key: value[i] for key, value in element[\"multiple_answers\"].items()}\n",
    "#                 correct_answer = {key: value[i] for key, value in element[\"correct_answers\"].items()}\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     idx = model(img, question, answer = multiple_answers, train=False, inference='rank_compressed', k_test=18).item()\n",
    "#                     predicted_answer = multiple_answers[\"input_ids\"][idx]\n",
    "#                     predicted_answer[0] = 30523\n",
    "#                     correct_answer = correct_answer[\"input_ids\"][0]\n",
    "#                     #print(f\"Predicted: {predicted_answer}, Correct: {correct_answer}\")\n",
    "#                     correct += int(np.array_equal(correct_answer.detach().cpu().numpy(), predicted_answer.detach().cpu().numpy()))\n",
    "#     correct /= size\n",
    "\n",
    "#     #Ploting results\n",
    "#     writer.add_scalar('Accuracy/test', correct*100, epoch)\n",
    "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%\")\n",
    "#     return 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters and optim\n",
    "import torch\n",
    "clip_value = 1.0\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping parameters\n",
    "n_epochs = 50\n",
    "early_stop_threshhold = 5\n",
    "best_accuracy = -1\n",
    "best_epoch = -1\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    folder_path = os.path.join(\"runs\", \"best_model\", currentModelIteration)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    torch.save(model.state_dict(), os.path.join(folder_path, filename))\n",
    "    \n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(os.path.join(\"runs\", \"checkpoint_SolvingCropping\", filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, optimizer, clip_value, epoch)\n",
    "    acc = eval(train_dataloader, model, epoch)\n",
    "    if acc>best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_epoch = epoch\n",
    "        checkpoint(model, \"best_model.pth\")\n",
    "    elif (epoch-best_epoch) > early_stop_threshhold:\n",
    "        print(\"--Early stopped training--\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
