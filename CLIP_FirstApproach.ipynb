{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os.path as path\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing image\n",
    "image_name_test = \"abstract_v002_val2015_000000020000.png\"\n",
    "image_id_test = int(image_name_test[:-4].split(\"_\")[-1])\n",
    "\n",
    "dataset_path=path.join(\"Images\")\n",
    "image = Image.open(path.join(dataset_path,image_name_test))\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What color is the fire?\n",
      "Multiple choices: ['white', 'red', 'yellow', 'red and orange', 'red orange', 'by bookshelf', '1', 'brown', 'blue', '4', 'yes', '3', 'orange', '2', 'no', 'foot rest', 'no cat', 'butt']\n"
     ]
    }
   ],
   "source": [
    "#Reading json file\n",
    "import json\n",
    " \n",
    "f = open('Annotations/MultipleChoice_abstract_v002_val2015_questions.json')\n",
    "data = json.load(f)\n",
    "\n",
    "for q in data['questions']:\n",
    "    image_id = q[\"image_id\"]\n",
    "    if image_id == image_id_test:\n",
    "        multiple_choices = q[\"multiple_choices\"]\n",
    "        question = q[\"question\"]\n",
    "        print(\"Question: \", question)\n",
    "        print(\"Multiple choices:\", multiple_choices)\n",
    "        break\n",
    "\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[49406,   768,  3140,   533,   518,  1769,   286, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   768,  3140,   533,   518,  1769,   286, 49407,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_tokens = processor(text=question, images=None, return_tensors=\"pt\", padding=True)\n",
    "print(question_tokens)\n",
    "question_emb = model.get_text_features(**question_tokens).detach().cpu().numpy()[0]\n",
    "print(len(question_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "img = processor(text=None, images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "image_emb = model.get_image_features(img).detach().cpu().numpy()[0]\n",
    "print(len(image_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posible answers embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 4])\n",
      "[[ 0.11625315 -0.02649451 -0.03693745 ... -0.06846023 -0.2037456\n",
      "   0.6314057 ]\n",
      " [ 0.13425508  0.0152376  -0.18170106 ... -0.06541064  0.1776331\n",
      "   0.26445475]\n",
      " [-0.19172315 -0.12251318  0.12002026 ... -0.15095477 -0.43429744\n",
      "  -0.38816124]\n",
      " ...\n",
      " [ 0.11757565  0.13806969  0.2414751  ... -0.48215938 -0.02207426\n",
      "   0.45251638]\n",
      " [ 0.14271183 -0.19830242  0.14422156 ... -0.10659659 -0.3477292\n",
      "   0.15352668]\n",
      " [ 0.04081094  0.27597252  0.35765576 ... -0.7412758   0.13738537\n",
      "   0.42976072]]\n"
     ]
    }
   ],
   "source": [
    "multiple_choices_tokens = processor(text=multiple_choices, images=None, return_tensors=\"pt\", padding=True)\n",
    "print(multiple_choices_tokens[\"input_ids\"].size())\n",
    "multiple_choices_emb = model.get_text_features(**multiple_choices_tokens).detach().cpu().numpy()\n",
    "print(multiple_choices_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29649082, 0.28999376, 0.3011317, 0.3811985, 0.3887719, 0.30011657, 0.3709505, 0.35827, 0.35228503, 0.3408517, 0.34658712, 0.3653583, 0.26324734, 0.2699174, 0.3405001, 0.29251245, 0.31902406, 0.36002368]\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "ComparableVector = image_emb * question_emb\n",
    "ComparableVector = ComparableVector / np.linalg.norm(ComparableVector)\n",
    "\n",
    "similarities=[]\n",
    "for choice_emb in multiple_choices_emb:\n",
    "    choice_emb = choice_emb / np.linalg.norm(choice_emb)\n",
    "    similarities.append(np.dot(ComparableVector, choice_emb) / (np.linalg.norm(ComparableVector)*np.linalg.norm(choice_emb)))\n",
    "print(similarities)\n",
    "argmax = np.argmax(similarities)\n",
    "print(multiple_choices[argmax])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
