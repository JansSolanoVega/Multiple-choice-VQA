{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset and Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Images:  97%|█████████▋| 292/300 [00:03<00:00, 112.08it/s]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from torch.utils.data import random_split\n",
    "sys.path.append('../')  # Adjust the path accordingly\n",
    "from VQA_Dataset_BLIP import VQA_Dataset\n",
    "\n",
    "dataset = VQA_Dataset()\n",
    "dataset.load_all(preprocess=None, length=4000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pytorch dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataset, text_processor, image_processor):\n",
    "        self.dataset = dataset\n",
    "        self.text_processor = text_processor\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = 32\n",
    "        self.image_height = 256\n",
    "        self.image_width = 256\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):   \n",
    "        # get image + text\n",
    "        answers = self.dataset[idx][3]\n",
    "        questions = self.dataset[idx][1]\n",
    "        image = self.dataset[idx][0].convert('RGB')\n",
    "\n",
    "        image_encoding = self.image_processor(image,\n",
    "                                  do_resize=True,\n",
    "                                  size=(self.image_height,self.image_width),\n",
    "                                  return_tensors=\"pt\")\n",
    "\n",
    "        encoding = self.text_processor(\n",
    "                                  None,\n",
    "                                  questions,\n",
    "                                  padding=\"max_length\",\n",
    "                                  truncation=True,\n",
    "                                  max_length = self.max_length,\n",
    "                                  return_tensors=\"pt\"\n",
    "                                  )\n",
    "        # # remove batch dimension\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        encoding[\"pixel_values\"] = image_encoding[\"pixel_values\"][0]\n",
    "        # # add labels\n",
    "        labels = self.text_processor.tokenizer.encode(\n",
    "            answers,\n",
    "            max_length= self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )[0]\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load processor\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import BlipProcessor, BlipImageProcessor, BlipForQuestionAnswering\n",
    "\n",
    "text_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "image_processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  240\n",
      "Test size:  30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 2515, 2009, 4025, 2066, 2016, 2003, 3403, 2005, 2060, 2111, 2000,\n",
       "        2272, 1029,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]), 'pixel_values': tensor([[[ 1.3172,  1.3172,  1.3172,  ...,  1.2880,  0.5143,  1.2442],\n",
       "         [ 1.3172,  1.3172,  1.3172,  ...,  0.7041,  0.4267,  1.3610],\n",
       "         [ 1.3172,  1.3172,  1.3172,  ...,  0.1493,  1.2150,  1.3318],\n",
       "         ...,\n",
       "         [-0.8288, -0.8288, -0.8288,  ..., -0.3470, -0.3470, -0.3324],\n",
       "         [-0.8288, -0.8288, -0.8288,  ..., -0.5368, -0.5660, -0.5660],\n",
       "         [-0.8288, -0.8288, -0.8288,  ..., -1.4711, -1.4711, -1.6025]],\n",
       "\n",
       "        [[ 2.0149,  2.0149,  2.0149,  ...,  1.9698,  1.0093,  1.9248],\n",
       "         [ 2.0149,  2.0149,  2.0149,  ...,  1.2645,  0.9343,  2.0449],\n",
       "         [ 2.0149,  2.0149,  2.0149,  ...,  0.5891,  1.8948,  2.0299],\n",
       "         ...,\n",
       "         [ 0.1089,  0.1089,  0.1089,  ..., -0.4914, -0.4764, -0.4914],\n",
       "         [ 0.1089,  0.1089,  0.1089,  ..., -0.6565, -0.6865, -0.6865],\n",
       "         [ 0.1089,  0.1089,  0.1089,  ..., -1.4820, -1.4669, -1.5720]],\n",
       "\n",
       "        [[ 2.1317,  2.1317,  2.1317,  ...,  2.0890,  1.1647,  2.0464],\n",
       "         [ 2.1317,  2.1317,  2.1317,  ...,  1.4065,  1.0936,  2.1317],\n",
       "         [ 2.1317,  2.1317,  2.1317,  ...,  0.7666,  2.0037,  2.1459],\n",
       "         ...,\n",
       "         [-1.4233, -1.4233, -1.4233,  ...,  0.1124,  0.1124,  0.1124],\n",
       "         [-1.4233, -1.4233, -1.4233,  ..., -0.1151, -0.1435, -0.1578],\n",
       "         [-1.4233, -1.4233, -1.4233,  ..., -1.1389, -1.1247, -1.2669]]]), 'labels': tensor([ 101, 2748,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pytorch dataset creation\n",
    "dataset_torch = VQADataset(dataset, text_processor, image_processor)\n",
    "\n",
    "train_size = int(len(dataset_torch)*0.8)\n",
    "val_size = int(len(dataset_torch)*0.1)\n",
    "test_size = int(len(dataset_torch))-train_size-val_size\n",
    "generator = torch.Generator().manual_seed(1)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_torch, [train_size, val_size, test_size], generator=generator)\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    pixel_values = [item['pixel_values'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    # create new batch\n",
    "    batch = {}\n",
    "    batch['input_ids'] = torch.stack(input_ids)\n",
    "    batch['attention_mask'] = torch.stack(attention_mask)\n",
    "    batch['pixel_values'] = torch.stack(pixel_values)\n",
    "    batch['labels'] = torch.stack(labels)\n",
    "\n",
    "    return batch\n",
    "\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              collate_fn=collate_fn,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            collate_fn=collate_fn,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([32, 32])\n",
      "attention_mask torch.Size([32, 32])\n",
      "pixel_values torch.Size([32, 3, 256, 256])\n",
      "labels torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "# batch_idx = 1\n",
    "# image_mean = image_processor.image_mean\n",
    "# image_std = image_processor.image_std\n",
    "# unnormalized_image = (batch[\"pixel_values\"][batch_idx].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\n",
    "# unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "# unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "\n",
    "# print(\"Question: \",text_processor.decode(batch[\"input_ids\"][batch_idx]))\n",
    "# print(\"Answer: \",text_processor.decode(batch[\"labels\"][batch_idx]))\n",
    "# plt.imshow(Image.fromarray(unnormalized_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os \n",
    "\n",
    "#Tensorboard\n",
    "currentModelIteration = \"5e-5_4k\"\n",
    "folder_path = os.path.join(\"runs\", \"trainingsBLIP\", currentModelIteration)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "writer = SummaryWriter(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, vqa_model, optimizer, clip_value, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    vqa_model.train()\n",
    "    train_correct_num, train_total, train_cost_acum = 0, 0, 0.0\n",
    "\n",
    "    for batch_i, batch in enumerate(dataloader):    \n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()      \n",
    "        outputs = vqa_model(**batch)\n",
    "        #predicted_answers = text_processor.batch_decode(outputs[0], skip_special_tokens=True)\n",
    "        #correct_answers = text_processor.batch_decode(batch[\"labels\"][0], skip_special_tokens=True)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vqa_model.parameters(), max_norm=clip_value, error_if_nonfinite=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Ploting results\n",
    "        train_cost_acum += loss\n",
    "        #train_correct_num += sum(pred_answer == correct_answer for pred_answer, correct_answer in zip(predicted_answers, correct_answers))\n",
    "        #train_total += len(correct_answers)\n",
    "\n",
    "        if batch_i % 50 == 1:\n",
    "            writer.add_scalar('Loss/training', float(train_cost_acum)/batch_i, epoch * size + batch_i)\n",
    "            #writer.add_scalar('Accuracy/training', float(train_correct_num)/train_total, epoch * size + batch_i)\n",
    "            loss, current = loss.item(), batch_i*len(batch_size)\n",
    "            print(\"loss: \", loss, current, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:    \n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "            outputs = model.generate(**batch)\n",
    "            predicted_answers = text_processor.batch_decode(outputs,skip_special_tokens=True)\n",
    "            correct_answers = text_processor.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "            correct += sum(pred_answer == correct_answer for pred_answer, correct_answer in zip(predicted_answers, correct_answers))   \n",
    "\n",
    "            #Debugging\n",
    "            # print(\"Question: \", text_processor.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)[0])      \n",
    "            # print(\"Answer: \", text_processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)[0])  \n",
    "            # print(\"Predicted: \", text_processor.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "                        \n",
    "    correct /= size\n",
    "\n",
    "    #Ploting results\n",
    "    writer.add_scalar('Accuracy/test', correct*100, epoch)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%\")\n",
    "    return 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForQuestionAnswering(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): BlipTextModel(\n",
       "    (embeddings): BlipTextEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): BlipTextEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BlipTextLayer(\n",
       "          (attention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BlipTextIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BlipTextOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameters and optim\n",
    "import torch\n",
    "\n",
    "clip_value = 1.0\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping parameters\n",
    "n_epochs = 50\n",
    "early_stop_threshhold = 5\n",
    "best_accuracy = -1\n",
    "best_epoch = -1\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    folder_path = os.path.join(\"runs\", \"best_model\", currentModelIteration)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    torch.save(model.state_dict(), os.path.join(folder_path, filename))\n",
    "    \n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(os.path.join(\"runs\", \"checkpoint_SolvingCropping\", filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, optimizer, clip_value, epoch)\n",
    "    acc = eval(val_dataloader, model, epoch)\n",
    "    if acc>best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_epoch = epoch\n",
    "        checkpoint(model, \"best_model.pth\")\n",
    "    elif (epoch-best_epoch) > early_stop_threshhold:\n",
    "        print(\"--Early stopped training--\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
