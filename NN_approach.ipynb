{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from VQA_Dataset import VQA_Dataset\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\\n\\ndataset = VQA_Dataset()\\ndataset.load_all(preprocess, length=3000, device=device)\\n\\n#Testing dataset\\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\\n\\nfor element in tqdm(dataloader, desc=\"Testing\"):\\n    ids_batch = element[\\'image_id\\']\\n    for i, id in enumerate(ids_batch):\\n        if id == 26216:\\n            print(element[\"question\"][i])#, \"question: \", i[\\'question_tokens\\'], \"answer: \", i[\\'answer_tokens\\'])'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from VQA_Dataset import VQA_Dataset\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "dataset = VQA_Dataset()\n",
    "dataset.load_all(preprocess, length=4000, device=device)\n",
    "\n",
    "#Testing dataset\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for element in tqdm(dataloader, desc=\"Testing\"):\n",
    "    ids_batch = element['image_id']\n",
    "    for i, id in enumerate(ids_batch):\n",
    "        if id == 26216:\n",
    "            print(element[\"question\"][i])#, \"question: \", i['question_tokens'], \"answer: \", i['answer_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test-train split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3200\n",
      "Test size:  400\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\10mociclo\\FoundationModels\\Multiple-choice-VQA\\NN_approach.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approach.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approach.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(\u001b[39m2\u001b[39m,\u001b[39mint\u001b[39m(batch_size\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m),i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approach.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     ax\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39mtranspose(element[\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m][i]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu(), (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))) \u001b[39m#Converting to matplotlib format\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approach.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     ax\u001b[39m.\u001b[39mset_title(\u001b[39m\"\u001b[39m\u001b[39mQuestion: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(element[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m][i])\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAnswer: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(element[\u001b[39m\"\u001b[39m\u001b[39mcorrect_answer_text\u001b[39m\u001b[39m\"\u001b[39m][i])\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(element[\u001b[39m\"\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m\"\u001b[39m][i]), fontsize\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approach.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_size = int(len(dataset)*0.8)\n",
    "val_size = int(len(dataset)*0.1)\n",
    "test_size = int(len(dataset))-train_size-val_size\n",
    "generator = torch.Generator().manual_seed(1)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)\n",
    "\n",
    "batch_size=2\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Checking data\n",
    "fig=plt.figure(0, (28, 10))\n",
    "for element in train_dataloader:\n",
    "    for i in range(batch_size):\n",
    "        ax = fig.add_subplot(2,int(batch_size/2),i+1)\n",
    "        \n",
    "        ax.imshow(np.transpose(element[\"image\"][i].squeeze(0).cpu(), (1, 2, 0))) #Converting to matplotlib format\n",
    "        ax.set_title(\"Question: \"+str(element[\"question\"][i])+\"\\nAnswer: \"+str(element[\"correct_answer_text\"][i])+\" \"+str(element[\"image_id\"][i]), fontsize=9)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VQA_Model4\n",
    "\n",
    "#CLIP\n",
    "clip_model, preprocess = clip.load('ViT-B/32', device)\n",
    "vqa_model = VQA_Model4(clip_model, device)\n",
    "\n",
    "# Freezing clip model: Without this, gradients scales were different (clip and mlp gradients), generated exploding gradient \n",
    "for param in vqa_model.model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image preprocessed:  torch.Size([1, 3, 224, 224])\n",
      "Image encoded size:  torch.Size([1, 512])\n",
      "Text tokenized size:  torch.Size([3, 77])\n",
      "Text encoded size:  torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "img = Image.open(os.path.join(\"Images\", \"abstract_v002_val2015_000000029903.png\"))\n",
    "image_input = preprocess(img).unsqueeze(0).to(device)\n",
    "print(\"Image preprocessed: \",image_input.shape)\n",
    "\n",
    "image_features = clip_model.encode_image(image_input)\n",
    "print(\"Image encoded size: \", image_features.shape)\n",
    "\n",
    "text = clip.tokenize([\"a diagram of the dof\" , \"a dog\", \"a cat\"]).to(device)\n",
    "print(\"Text tokenized size: \",text.shape)\n",
    "\n",
    "text_features = clip_model.encode_text(text)\n",
    "print(\"Text encoded size: \",text_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training & Optim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, vqa_model, loss_function, optimizer, clip_value):\n",
    "    size = len(dataloader.dataset)\n",
    "    vqa_model.train()\n",
    "\n",
    "    for batch, (data) in enumerate(dataloader):    \n",
    "        images = data[\"image\"].squeeze(1)\n",
    "        question_tokens = data[\"question_tokens\"].squeeze(1)\n",
    "        answer_tokens = data[\"answer_tokens\"].squeeze(1)\n",
    "        similarity_pred = vqa_model(images, question_tokens, answer_tokens)\n",
    "\n",
    "        similarity_label_arg = data[\"correct_answer_idx\"].to(device)\n",
    "        \n",
    "        loss = loss_function(similarity_pred, similarity_label_arg)\n",
    "\n",
    "        # #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(vqa_model.parameters(), max_norm=clip_value, error_if_nonfinite=True)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 ==0:\n",
    "            loss, current = loss.item(), batch*len(images)\n",
    "            print(\"loss: \", loss, current, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model, loss_function):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:    \n",
    "            images = data[\"image\"].squeeze(1)\n",
    "            question_tokens = data[\"question_tokens\"].squeeze(1)\n",
    "            answer_tokens = data[\"answer_tokens\"].squeeze(1)\n",
    "            similarity_pred = model(images, question_tokens, answer_tokens)\n",
    "\n",
    "            similarity_label_arg = torch.tensor(data[\"correct_answer_idx\"]).to(device)\n",
    "            \n",
    "            val_loss += loss_function(similarity_pred, similarity_label_arg)\n",
    "            correct += (similarity_pred.argmax(1) == similarity_label_arg).type(torch.float).sum().item()\n",
    "            \n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    return 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters and optim\n",
    "from torch import nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "clip_value = 1.0\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(vqa_model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateu(optimizer, patience=5, verbose=True) #Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping parameters\n",
    "import os\n",
    "\n",
    "n_epochs = 50\n",
    "early_stop_threshhold = 5\n",
    "best_accuracy = -1\n",
    "best_epoch = -1\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    folder_path = os.path.join(\"runs\", \"checkpoint_SolvingCropping\")\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    torch.save(model.state_dict(), os.path.join(folder_path, filename))\n",
    "    \n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(os.path.join(\"runs\", \"checkpoint_SolvingCropping\", filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss:  2.749495267868042 0 2400\n",
      "loss:  2.4484424591064453 100 2400\n",
      "loss:  1.5176622867584229 200 2400\n",
      "loss:  3.8358585834503174 300 2400\n",
      "loss:  1.4069647789001465 400 2400\n",
      "loss:  1.5639595985412598 500 2400\n",
      "loss:  0.8599292635917664 600 2400\n",
      "loss:  1.6704210042953491 700 2400\n",
      "loss:  1.8656642436981201 800 2400\n",
      "loss:  1.3267436027526855 900 2400\n",
      "loss:  1.8402092456817627 1000 2400\n",
      "loss:  1.0737415552139282 1100 2400\n",
      "loss:  0.8069823980331421 1200 2400\n",
      "loss:  1.6569902896881104 1300 2400\n",
      "loss:  2.107916831970215 1400 2400\n",
      "loss:  2.088829278945923 1500 2400\n",
      "loss:  1.3075658082962036 1600 2400\n",
      "loss:  1.3777270317077637 1700 2400\n",
      "loss:  1.7279810905456543 1800 2400\n",
      "loss:  2.141223669052124 1900 2400\n",
      "loss:  1.842322826385498 2000 2400\n",
      "loss:  4.026528835296631 2100 2400\n",
      "loss:  3.2461729049682617 2200 2400\n",
      "loss:  0.9547348618507385 2300 2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zetans\\AppData\\Local\\Temp\\ipykernel_14016\\1468784600.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  similarity_label_arg = torch.tensor(data[\"correct_answer_idx\"]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 42.3%, Avg loss: 1.748039 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss:  0.9593484401702881 0 2400\n",
      "loss:  0.6302608847618103 100 2400\n",
      "loss:  0.9160096645355225 200 2400\n",
      "loss:  0.8039377927780151 300 2400\n",
      "loss:  3.3930606842041016 400 2400\n",
      "loss:  1.9742428064346313 500 2400\n",
      "loss:  3.1940197944641113 600 2400\n",
      "loss:  1.29179847240448 700 2400\n",
      "loss:  1.7889981269836426 800 2400\n",
      "loss:  0.609059751033783 900 2400\n",
      "loss:  1.081984519958496 1000 2400\n",
      "loss:  1.952327013015747 1100 2400\n",
      "loss:  1.9396013021469116 1200 2400\n",
      "loss:  0.846930742263794 1300 2400\n",
      "loss:  0.582335352897644 1400 2400\n",
      "loss:  1.3503772020339966 1500 2400\n",
      "loss:  1.938119649887085 1600 2400\n",
      "loss:  2.3272695541381836 1700 2400\n",
      "loss:  0.7953585386276245 1800 2400\n",
      "loss:  1.6516739130020142 1900 2400\n",
      "loss:  2.0259063243865967 2000 2400\n",
      "loss:  1.8585820198059082 2100 2400\n",
      "loss:  0.9327870607376099 2200 2400\n",
      "loss:  3.569924831390381 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 1.617954 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss:  3.14839506149292 0 2400\n",
      "loss:  0.6683210134506226 100 2400\n",
      "loss:  1.3253263235092163 200 2400\n",
      "loss:  1.272613763809204 300 2400\n",
      "loss:  2.015982151031494 400 2400\n",
      "loss:  2.189269542694092 500 2400\n",
      "loss:  2.271087884902954 600 2400\n",
      "loss:  0.43998265266418457 700 2400\n",
      "loss:  0.5805308222770691 800 2400\n",
      "loss:  0.6943016052246094 900 2400\n",
      "loss:  1.0813872814178467 1000 2400\n",
      "loss:  0.741447925567627 1100 2400\n",
      "loss:  1.1216514110565186 1200 2400\n",
      "loss:  1.4849070310592651 1300 2400\n",
      "loss:  1.4279574155807495 1400 2400\n",
      "loss:  0.9708368182182312 1500 2400\n",
      "loss:  0.7716413140296936 1600 2400\n",
      "loss:  0.900138795375824 1700 2400\n",
      "loss:  0.6292713284492493 1800 2400\n",
      "loss:  1.0021573305130005 1900 2400\n",
      "loss:  1.1315903663635254 2000 2400\n",
      "loss:  0.44516006112098694 2100 2400\n",
      "loss:  0.8486711978912354 2200 2400\n",
      "loss:  0.6560384631156921 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.571508 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss:  1.0389385223388672 0 2400\n",
      "loss:  1.8555845022201538 100 2400\n",
      "loss:  0.2324988692998886 200 2400\n",
      "loss:  0.6217360496520996 300 2400\n",
      "loss:  0.5169335603713989 400 2400\n",
      "loss:  2.46466326713562 500 2400\n",
      "loss:  1.7495836019515991 600 2400\n",
      "loss:  0.28497985005378723 700 2400\n",
      "loss:  0.4574432969093323 800 2400\n",
      "loss:  1.9665992259979248 900 2400\n",
      "loss:  0.39758825302124023 1000 2400\n",
      "loss:  1.114827275276184 1100 2400\n",
      "loss:  1.1837595701217651 1200 2400\n",
      "loss:  0.6468213200569153 1300 2400\n",
      "loss:  0.34786954522132874 1400 2400\n",
      "loss:  4.760553359985352 1500 2400\n",
      "loss:  1.248807668685913 1600 2400\n",
      "loss:  0.5272183418273926 1700 2400\n",
      "loss:  1.3902097940444946 1800 2400\n",
      "loss:  0.8125209808349609 1900 2400\n",
      "loss:  1.7906584739685059 2000 2400\n",
      "loss:  1.8141919374465942 2100 2400\n",
      "loss:  3.151285171508789 2200 2400\n",
      "loss:  0.8244470357894897 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.545905 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss:  1.921546220779419 0 2400\n",
      "loss:  1.8574002981185913 100 2400\n",
      "loss:  0.6877688765525818 200 2400\n",
      "loss:  0.20187410712242126 300 2400\n",
      "loss:  0.9515944719314575 400 2400\n",
      "loss:  0.657694399356842 500 2400\n",
      "loss:  0.406105637550354 600 2400\n",
      "loss:  1.29482102394104 700 2400\n",
      "loss:  0.3703562021255493 800 2400\n",
      "loss:  0.7430993318557739 900 2400\n",
      "loss:  0.19094814360141754 1000 2400\n",
      "loss:  0.37855982780456543 1100 2400\n",
      "loss:  0.5057310461997986 1200 2400\n",
      "loss:  0.3209612965583801 1300 2400\n",
      "loss:  0.6074406504631042 1400 2400\n",
      "loss:  3.4831531047821045 1500 2400\n",
      "loss:  1.351883053779602 1600 2400\n",
      "loss:  0.5272047519683838 1700 2400\n",
      "loss:  0.5495809316635132 1800 2400\n",
      "loss:  1.6053106784820557 1900 2400\n",
      "loss:  0.2071818858385086 2000 2400\n",
      "loss:  0.30590689182281494 2100 2400\n",
      "loss:  0.5143235325813293 2200 2400\n",
      "loss:  2.3748021125793457 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.671337 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss:  0.5637632012367249 0 2400\n",
      "loss:  0.34625154733657837 100 2400\n",
      "loss:  0.8859658241271973 200 2400\n",
      "loss:  1.1332528591156006 300 2400\n",
      "loss:  0.9859600067138672 400 2400\n",
      "loss:  0.2083882838487625 500 2400\n",
      "loss:  0.016723915934562683 600 2400\n",
      "loss:  3.9828734397888184 700 2400\n",
      "loss:  0.34664687514305115 800 2400\n",
      "loss:  0.7466685771942139 900 2400\n",
      "loss:  0.7691637277603149 1000 2400\n",
      "loss:  0.2915646433830261 1100 2400\n",
      "loss:  0.7555829882621765 1200 2400\n",
      "loss:  0.045682504773139954 1300 2400\n",
      "loss:  0.3556489050388336 1400 2400\n",
      "loss:  0.1933516561985016 1500 2400\n",
      "loss:  1.018169641494751 1600 2400\n",
      "loss:  0.23014430701732635 1700 2400\n",
      "loss:  0.2944204807281494 1800 2400\n",
      "loss:  1.3307716846466064 1900 2400\n",
      "loss:  0.4598369598388672 2000 2400\n",
      "loss:  0.8558610677719116 2100 2400\n",
      "loss:  0.6455581784248352 2200 2400\n",
      "loss:  0.29248422384262085 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.691500 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss:  0.5599572658538818 0 2400\n",
      "loss:  0.14458200335502625 100 2400\n",
      "loss:  0.8185136914253235 200 2400\n",
      "loss:  0.057100459933280945 300 2400\n",
      "loss:  0.12518414855003357 400 2400\n",
      "loss:  0.47953253984451294 500 2400\n",
      "loss:  0.17072869837284088 600 2400\n",
      "loss:  0.625272274017334 700 2400\n",
      "loss:  0.06836766004562378 800 2400\n",
      "loss:  0.5288538932800293 900 2400\n",
      "loss:  0.49901100993156433 1000 2400\n",
      "loss:  0.40715259313583374 1100 2400\n",
      "loss:  0.1778969019651413 1200 2400\n",
      "loss:  0.4798775613307953 1300 2400\n",
      "loss:  0.23583205044269562 1400 2400\n",
      "loss:  1.8262231349945068 1500 2400\n",
      "loss:  0.20711331069469452 1600 2400\n",
      "loss:  0.14813974499702454 1700 2400\n",
      "loss:  0.20442163944244385 1800 2400\n",
      "loss:  0.30731308460235596 1900 2400\n",
      "loss:  0.41091829538345337 2000 2400\n",
      "loss:  1.9681686162948608 2100 2400\n",
      "loss:  0.14642910659313202 2200 2400\n",
      "loss:  0.44510024785995483 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.714700 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss:  1.2760827541351318 0 2400\n",
      "loss:  0.0090249078348279 100 2400\n",
      "loss:  0.1426890641450882 200 2400\n",
      "loss:  2.2979202270507812 300 2400\n",
      "loss:  0.7412258386611938 400 2400\n",
      "loss:  0.03884647414088249 500 2400\n",
      "loss:  0.12527860701084137 600 2400\n",
      "loss:  0.12000812590122223 700 2400\n",
      "loss:  0.4362407326698303 800 2400\n",
      "loss:  0.13067516684532166 900 2400\n",
      "loss:  0.08178578317165375 1000 2400\n",
      "loss:  0.04462497681379318 1100 2400\n",
      "loss:  1.441050410270691 1200 2400\n",
      "loss:  0.24282300472259521 1300 2400\n",
      "loss:  0.10281512141227722 1400 2400\n",
      "loss:  0.19231684505939484 1500 2400\n",
      "loss:  1.0566153526306152 1600 2400\n",
      "loss:  0.9277476668357849 1700 2400\n",
      "loss:  0.296274870634079 1800 2400\n",
      "loss:  0.15696817636489868 1900 2400\n",
      "loss:  0.01786816492676735 2000 2400\n",
      "loss:  0.4555710256099701 2100 2400\n",
      "loss:  1.1848342418670654 2200 2400\n",
      "loss:  0.9571993350982666 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 47.3%, Avg loss: 1.745706 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss:  0.06242773309350014 0 2400\n",
      "loss:  0.05348452925682068 100 2400\n",
      "loss:  0.08771495521068573 200 2400\n",
      "loss:  0.31443583965301514 300 2400\n",
      "loss:  0.8035339117050171 400 2400\n",
      "loss:  0.5519817471504211 500 2400\n",
      "loss:  0.8530255556106567 600 2400\n",
      "loss:  0.6222134828567505 700 2400\n",
      "loss:  0.5932825803756714 800 2400\n",
      "loss:  0.9836384057998657 900 2400\n",
      "loss:  1.6908845901489258 1000 2400\n",
      "loss:  0.09840037673711777 1100 2400\n",
      "loss:  0.387468159198761 1200 2400\n",
      "loss:  1.4299999475479126 1300 2400\n",
      "loss:  0.4688788652420044 1400 2400\n",
      "loss:  0.0702175498008728 1500 2400\n",
      "loss:  0.12477121502161026 1600 2400\n",
      "loss:  0.19622528553009033 1700 2400\n",
      "loss:  0.982272207736969 1800 2400\n",
      "loss:  2.0776076316833496 1900 2400\n",
      "loss:  0.12682324647903442 2000 2400\n",
      "loss:  0.022171391174197197 2100 2400\n",
      "loss:  0.4361640214920044 2200 2400\n",
      "loss:  0.32240575551986694 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.929844 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss:  0.2445179522037506 0 2400\n",
      "loss:  0.16651135683059692 100 2400\n",
      "loss:  0.04291096329689026 200 2400\n",
      "loss:  0.31175097823143005 300 2400\n",
      "loss:  1.4581133127212524 400 2400\n",
      "loss:  0.10728371143341064 500 2400\n",
      "loss:  0.31084030866622925 600 2400\n",
      "loss:  0.5634061098098755 700 2400\n",
      "loss:  0.027357082813978195 800 2400\n",
      "loss:  0.0811183974146843 900 2400\n",
      "loss:  2.0170931816101074 1000 2400\n",
      "loss:  1.47860848903656 1100 2400\n",
      "loss:  0.1306256204843521 1200 2400\n",
      "loss:  0.5048475861549377 1300 2400\n",
      "loss:  0.18227645754814148 1400 2400\n",
      "loss:  0.01665407419204712 1500 2400\n",
      "loss:  0.18337227404117584 1600 2400\n",
      "loss:  1.100813865661621 1700 2400\n",
      "loss:  0.002183604286983609 1800 2400\n",
      "loss:  0.028096795082092285 1900 2400\n",
      "loss:  0.1840878427028656 2000 2400\n",
      "loss:  1.2851155996322632 2100 2400\n",
      "loss:  0.986580491065979 2200 2400\n",
      "loss:  0.11024443060159683 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 2.006515 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss:  0.020760037004947662 0 2400\n",
      "loss:  0.33185407519340515 100 2400\n",
      "loss:  0.011225384660065174 200 2400\n",
      "loss:  0.030340636149048805 300 2400\n",
      "loss:  0.35480841994285583 400 2400\n",
      "loss:  0.02843271568417549 500 2400\n",
      "loss:  0.019635029137134552 600 2400\n",
      "loss:  0.5040936470031738 700 2400\n",
      "loss:  0.37729573249816895 800 2400\n",
      "loss:  0.6538373231887817 900 2400\n",
      "loss:  0.003886604215949774 1000 2400\n",
      "loss:  0.10347707569599152 1100 2400\n",
      "loss:  0.12056160718202591 1200 2400\n",
      "loss:  0.006720914971083403 1300 2400\n",
      "loss:  0.0059821465983986855 1400 2400\n",
      "loss:  1.1162916421890259 1500 2400\n",
      "loss:  0.016687611117959023 1600 2400\n",
      "loss:  0.00025173803442157805 1700 2400\n",
      "loss:  0.02400079369544983 1800 2400\n",
      "loss:  0.6906836032867432 1900 2400\n",
      "loss:  1.1385589838027954 2000 2400\n",
      "loss:  0.05589018017053604 2100 2400\n",
      "loss:  0.04642820730805397 2200 2400\n",
      "loss:  0.4336959719657898 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 2.156336 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss:  0.02875923551619053 0 2400\n",
      "loss:  0.09181053936481476 100 2400\n",
      "loss:  0.3399103283882141 200 2400\n",
      "loss:  0.005384738557040691 300 2400\n",
      "loss:  0.4202214777469635 400 2400\n",
      "loss:  0.01893741637468338 500 2400\n",
      "loss:  0.5452525019645691 600 2400\n",
      "loss:  0.3816143274307251 700 2400\n",
      "loss:  0.003272456582635641 800 2400\n",
      "loss:  0.00215252791531384 900 2400\n",
      "loss:  0.0428248830139637 1000 2400\n",
      "loss:  0.008636465296149254 1100 2400\n",
      "loss:  1.1973587274551392 1200 2400\n",
      "loss:  0.06996852904558182 1300 2400\n",
      "loss:  0.030528200790286064 1400 2400\n",
      "loss:  0.5850459337234497 1500 2400\n",
      "loss:  0.35696274042129517 1600 2400\n",
      "loss:  0.02110128290951252 1700 2400\n",
      "loss:  1.190943956375122 1800 2400\n",
      "loss:  0.01187749020755291 1900 2400\n",
      "loss:  0.013520626351237297 2000 2400\n",
      "loss:  0.013678907416760921 2100 2400\n",
      "loss:  0.008858944289386272 2200 2400\n",
      "loss:  0.006192801520228386 2300 2400\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 2.187717 \n",
      "\n",
      "--Early stopped training--\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, vqa_model, loss_fn, optimizer, clip_value)\n",
    "    acc = eval(val_dataloader, vqa_model, loss_fn)\n",
    "    if acc>best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_epoch = epoch\n",
    "        checkpoint(vqa_model, \"best_model.pth\")\n",
    "    elif (epoch-best_epoch) > early_stop_threshhold:\n",
    "        print(\"--Early stopped training--\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
