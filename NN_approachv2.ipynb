{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset with encodings already computed\n",
    "from VQA_Datasetv2 import VQA_Dataset_preloaded\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "dataset = VQA_Dataset_preloaded()\n",
    "\n",
    "#Computing\n",
    "#dataset.compute_store(preprocess, model, device, \"full\", length=4000)\n",
    "\n",
    "#Loading h5 file\n",
    "dataset.load(\"full\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test-train split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3200\n",
      "Test size:  400\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_size = int(len(dataset)*0.8)\n",
    "val_size = int(len(dataset)*0.1)\n",
    "test_size = int(len(dataset))-train_size-val_size\n",
    "generator = torch.Generator().manual_seed(1)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)\n",
    "\n",
    "batch_size=2\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VQA_Model_Precalc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os \n",
    "\n",
    "#Tensorboard\n",
    "currentModelIteration = \"1e-4_relu_hidden\"\n",
    "folder_path = os.path.join(\"runs\", \"trainings\", currentModelIteration)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "writer = SummaryWriter(folder_path)\n",
    "\n",
    "#CLIP\n",
    "clip_model, preprocess = clip.load('ViT-B/32', device)\n",
    "vqa_model = VQA_Model_Precalc(clip_model, device)\n",
    "\n",
    "# Freezing clip model: Without this, gradients scales were different (clip and mlp gradients), generated exploding gradient \n",
    "for param in vqa_model.model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image preprocessed:  torch.Size([1, 3, 224, 224])\n",
      "Image encoded size:  torch.Size([1, 512])\n",
      "Text tokenized size:  torch.Size([3, 77])\n",
      "Text encoded size:  torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "img = Image.open(os.path.join(\"Images\", \"abstract_v002_val2015_000000029903.png\"))\n",
    "image_input = preprocess(img).unsqueeze(0).to(device)\n",
    "print(\"Image preprocessed: \",image_input.shape)\n",
    "\n",
    "image_features = clip_model.encode_image(image_input)\n",
    "print(\"Image encoded size: \", image_features.shape)\n",
    "\n",
    "text = clip.tokenize([\"a diagram of the dof\" , \"a dog\", \"a cat\"]).to(device)\n",
    "print(\"Text tokenized size: \",text.shape)\n",
    "\n",
    "text_features = clip_model.encode_text(text)\n",
    "print(\"Text encoded size: \",text_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training & Optim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, vqa_model, loss_function, optimizer, clip_value, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    vqa_model.train()\n",
    "    train_cost_acum = 0.0\n",
    "\n",
    "    for batch, (data) in enumerate(dataloader):    \n",
    "        images = data[0].squeeze(1)\n",
    "        question_tokens = data[2].squeeze(1)\n",
    "        answer_tokens = data[1].squeeze(1)\n",
    "        similarity_pred = vqa_model(images, question_tokens, answer_tokens)\n",
    "\n",
    "        similarity_label_arg = data[3].squeeze(1).long().to(device)\n",
    "        \n",
    "        loss = loss_function(similarity_pred, similarity_label_arg)\n",
    "\n",
    "        #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(vqa_model.parameters(), max_norm=clip_value, error_if_nonfinite=True)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Ploting results\n",
    "        train_cost_acum += loss\n",
    "        if batch % 50 == 1:\n",
    "            writer.add_scalar('training loss', float(train_cost_acum)/batch, epoch * size + batch) #len(dataloader) returns total number of batchs in an epoch\n",
    "            loss, current = loss.item(), batch*len(images)\n",
    "            print(\"loss: \", loss, current, size)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, model, loss_function, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:    \n",
    "            images = data[0].squeeze(1).to(device)\n",
    "            question_tokens = data[2].squeeze(1).to(device)\n",
    "            answer_tokens = data[1].squeeze(1).to(device)\n",
    "            similarity_pred = model(images, question_tokens, answer_tokens)\n",
    "\n",
    "            similarity_label_arg = torch.tensor(data[3]).long().squeeze(1).to(device)\n",
    "            \n",
    "            val_loss += loss_function(similarity_pred, similarity_label_arg)\n",
    "            correct += (similarity_pred.argmax(1) == similarity_label_arg).type(torch.float).sum().item()\n",
    "            \n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    #Ploting results\n",
    "    writer.add_scalar('Accuracy/test', correct*100, epoch)\n",
    "    writer.add_scalar('Loss/test', val_loss, epoch)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    return 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters and optim\n",
    "from torch import nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "clip_value = 1.0\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(vqa_model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateu(optimizer, patience=5, verbose=True) #Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping parameters\n",
    "import os\n",
    "\n",
    "n_epochs = 50\n",
    "early_stop_threshhold = 5\n",
    "best_accuracy = -1\n",
    "best_epoch = -1\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    folder_path = os.path.join(\"runs\", \"best_model\", currentModelIteration)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    torch.save(model.state_dict(), os.path.join(folder_path, filename))\n",
    "    \n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(os.path.join(\"runs\", \"checkpoint_SolvingCropping\", filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss:  15.603597640991211 2 3200\n",
      "loss:  4.96431827545166 102 3200\n",
      "loss:  4.798961639404297 202 3200\n",
      "loss:  9.406116485595703 302 3200\n",
      "loss:  0.8838605880737305 402 3200\n",
      "loss:  9.867145538330078 502 3200\n",
      "loss:  6.355923175811768 602 3200\n",
      "loss:  1.0071303844451904 702 3200\n",
      "loss:  8.484251022338867 802 3200\n",
      "loss:  0.28028714656829834 902 3200\n",
      "loss:  0.9410175085067749 1002 3200\n",
      "loss:  0.5865575671195984 1102 3200\n",
      "loss:  0.7346488833427429 1202 3200\n",
      "loss:  1.618565320968628 1302 3200\n",
      "loss:  3.8468565940856934 1402 3200\n",
      "loss:  1.0159099102020264 1502 3200\n",
      "loss:  1.8026976585388184 1602 3200\n",
      "loss:  0.8143121004104614 1702 3200\n",
      "loss:  4.967896938323975 1802 3200\n",
      "loss:  2.2322518825531006 1902 3200\n",
      "loss:  2.0141613483428955 2002 3200\n",
      "loss:  1.2832977771759033 2102 3200\n",
      "loss:  2.808457851409912 2202 3200\n",
      "loss:  2.4589812755584717 2302 3200\n",
      "loss:  1.4496240615844727 2402 3200\n",
      "loss:  1.3651671409606934 2502 3200\n",
      "loss:  4.000920295715332 2602 3200\n",
      "loss:  1.0072696208953857 2702 3200\n",
      "loss:  6.058669090270996 2802 3200\n",
      "loss:  1.8321435451507568 2902 3200\n",
      "loss:  0.9679967164993286 3002 3200\n",
      "loss:  0.8969773650169373 3102 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zetans\\AppData\\Local\\Temp\\ipykernel_13660\\1588012289.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  similarity_label_arg = torch.tensor(data[3]).long().squeeze(1).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.083412 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss:  0.11003571003675461 2 3200\n",
      "loss:  2.526693820953369 102 3200\n",
      "loss:  0.5331398844718933 202 3200\n",
      "loss:  0.6288391351699829 302 3200\n",
      "loss:  1.216438889503479 402 3200\n",
      "loss:  2.254531145095825 502 3200\n",
      "loss:  1.4811255931854248 602 3200\n",
      "loss:  2.754563093185425 702 3200\n",
      "loss:  2.7307422161102295 802 3200\n",
      "loss:  3.2887728214263916 902 3200\n",
      "loss:  0.08047059178352356 1002 3200\n",
      "loss:  2.499587297439575 1102 3200\n",
      "loss:  0.1906214952468872 1202 3200\n",
      "loss:  3.547283172607422 1302 3200\n",
      "loss:  0.9744843244552612 1402 3200\n",
      "loss:  1.8764451742172241 1502 3200\n",
      "loss:  1.4640984535217285 1602 3200\n",
      "loss:  4.074671745300293 1702 3200\n",
      "loss:  1.1250826120376587 1802 3200\n",
      "loss:  0.5008111000061035 1902 3200\n",
      "loss:  0.0626831203699112 2002 3200\n",
      "loss:  1.6606991291046143 2102 3200\n",
      "loss:  1.192213773727417 2202 3200\n",
      "loss:  0.36414051055908203 2302 3200\n",
      "loss:  0.10931529104709625 2402 3200\n",
      "loss:  1.751401424407959 2502 3200\n",
      "loss:  0.5987206101417542 2602 3200\n",
      "loss:  2.274676561355591 2702 3200\n",
      "loss:  0.32788825035095215 2802 3200\n",
      "loss:  1.6342653036117554 2902 3200\n",
      "loss:  0.075400710105896 3002 3200\n",
      "loss:  1.8446412086486816 3102 3200\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 2.036683 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss:  1.1687018871307373 2 3200\n",
      "loss:  0.367900013923645 102 3200\n",
      "loss:  0.3487589955329895 202 3200\n",
      "loss:  0.31539610028266907 302 3200\n",
      "loss:  0.7463698387145996 402 3200\n",
      "loss:  0.5916882157325745 502 3200\n",
      "loss:  1.1481876373291016 602 3200\n",
      "loss:  1.2661008834838867 702 3200\n",
      "loss:  0.47794800996780396 802 3200\n",
      "loss:  1.2426974773406982 902 3200\n",
      "loss:  3.0560154914855957 1002 3200\n",
      "loss:  1.302649736404419 1102 3200\n",
      "loss:  0.4929967224597931 1202 3200\n",
      "loss:  2.018540620803833 1302 3200\n",
      "loss:  5.909102439880371 1402 3200\n",
      "loss:  0.6769275069236755 1502 3200\n",
      "loss:  0.6097699403762817 1602 3200\n",
      "loss:  1.0556000471115112 1702 3200\n",
      "loss:  0.19876669347286224 1802 3200\n",
      "loss:  0.03387519344687462 1902 3200\n",
      "loss:  0.27386632561683655 2002 3200\n",
      "loss:  0.911662220954895 2102 3200\n",
      "loss:  1.8114882707595825 2202 3200\n",
      "loss:  1.3376597166061401 2302 3200\n",
      "loss:  2.0429553985595703 2402 3200\n",
      "loss:  0.8995814323425293 2502 3200\n",
      "loss:  2.386934280395508 2602 3200\n",
      "loss:  1.1967765092849731 2702 3200\n",
      "loss:  0.17926594614982605 2802 3200\n",
      "loss:  0.9099349975585938 2902 3200\n",
      "loss:  2.7186007499694824 3002 3200\n",
      "loss:  0.17319165170192719 3102 3200\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 2.349577 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss:  0.15740063786506653 2 3200\n",
      "loss:  0.04850854352116585 102 3200\n",
      "loss:  0.19339808821678162 202 3200\n",
      "loss:  3.5827741622924805 302 3200\n",
      "loss:  0.5837281346321106 402 3200\n",
      "loss:  3.9204392433166504 502 3200\n",
      "loss:  0.7097769379615784 602 3200\n",
      "loss:  0.1444041132926941 702 3200\n",
      "loss:  0.7857661247253418 802 3200\n",
      "loss:  0.2661067843437195 902 3200\n",
      "loss:  0.05893142893910408 1002 3200\n",
      "loss:  0.016200020909309387 1102 3200\n",
      "loss:  0.6796782612800598 1202 3200\n",
      "loss:  0.43205520510673523 1302 3200\n",
      "loss:  0.5448659062385559 1402 3200\n",
      "loss:  0.5208951830863953 1502 3200\n",
      "loss:  2.369586706161499 1602 3200\n",
      "loss:  0.382793128490448 1702 3200\n",
      "loss:  0.1294945925474167 1802 3200\n",
      "loss:  1.7935816049575806 1902 3200\n",
      "loss:  0.16911062598228455 2002 3200\n",
      "loss:  1.1349616050720215 2102 3200\n",
      "loss:  0.8008317351341248 2202 3200\n",
      "loss:  0.6823861598968506 2302 3200\n",
      "loss:  0.06216730177402496 2402 3200\n",
      "loss:  0.8495442271232605 2502 3200\n",
      "loss:  0.6093991994857788 2602 3200\n",
      "loss:  0.8098652958869934 2702 3200\n",
      "loss:  1.12954580783844 2802 3200\n",
      "loss:  0.09672818332910538 2902 3200\n",
      "loss:  0.09754779189825058 3002 3200\n",
      "loss:  0.06415653228759766 3102 3200\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.535190 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss:  0.13227218389511108 2 3200\n",
      "loss:  0.056626155972480774 102 3200\n",
      "loss:  1.8511004447937012 202 3200\n",
      "loss:  1.0515809059143066 302 3200\n",
      "loss:  1.3444890975952148 402 3200\n",
      "loss:  0.009495675563812256 502 3200\n",
      "loss:  0.6758472323417664 602 3200\n",
      "loss:  0.0008273838902823627 702 3200\n",
      "loss:  3.871288776397705 802 3200\n",
      "loss:  0.028547881171107292 902 3200\n",
      "loss:  3.179581642150879 1002 3200\n",
      "loss:  0.6810440421104431 1102 3200\n",
      "loss:  1.3111913204193115 1202 3200\n",
      "loss:  1.2571903467178345 1302 3200\n",
      "loss:  2.2895545959472656 1402 3200\n",
      "loss:  0.3056592643260956 1502 3200\n",
      "loss:  0.5436511039733887 1602 3200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\10mociclo\\FoundationModels\\Multiple-choice-VQA\\NN_approachv2.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train(train_dataloader, vqa_model, loss_fn, optimizer, clip_value, epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     acc \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m(val_dataloader, vqa_model, loss_fn, epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m acc\u001b[39m>\u001b[39mbest_accuracy:\n",
      "\u001b[1;32me:\\10mociclo\\FoundationModels\\Multiple-choice-VQA\\NN_approachv2.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(vqa_model\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39mclip_value, error_if_nonfinite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#Ploting results\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/10mociclo/FoundationModels/Multiple-choice-VQA/NN_approachv2.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m train_cost_acum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\Zetans\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zetans\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Zetans\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Zetans\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\Zetans\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py:496\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    493\u001b[0m device_params \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mview_as_real(x) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m device_params]\n\u001b[0;32m    495\u001b[0m \u001b[39m# update steps\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m torch\u001b[39m.\u001b[39;49m_foreach_add_(device_state_steps, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    498\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    499\u001b[0m     \u001b[39m# Re-use the intermediate memory (device_grads) already allocated for maximize\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[39mif\u001b[39;00m maximize:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, vqa_model, loss_fn, optimizer, clip_value, epoch)\n",
    "    acc = eval(val_dataloader, vqa_model, loss_fn, epoch)\n",
    "    if acc>best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_epoch = epoch\n",
    "        checkpoint(vqa_model, \"best_model.pth\")\n",
    "    elif (epoch-best_epoch) > early_stop_threshhold:\n",
    "        print(\"--Early stopped training--\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
