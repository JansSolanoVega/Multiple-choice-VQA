{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Compose\n",
      "clip model loaded\n"
     ]
    }
   ],
   "source": [
    "from VQA_Dataset_CLIP import VQA_Dataset, VQA_Dataset_preloaded, VQA_Dataset_Sentences\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(preprocess.__class__.__name__)\n",
    "model.to(torch.float32)\n",
    "print(\"clip model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "  (visual): VisionTransformer(\n",
      "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): Sequential(\n",
      "        (0): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): ResidualAttentionBlock(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Precalc clip embedding to train faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_abstract_train = VQA_Dataset_preloaded()\n",
    "#dataset_abstract_train.compute_store(preprocess, model, device, \"dataset_abstract_train_60k_sentencesv2\", name=\"train\", length=60000, mode=\"scale\", real=False, sentences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_real = VQA_Dataset_preloaded()\n",
    "dataset_abstract_test = VQA_Dataset_preloaded()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Only load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "#dataset_train = VQA_Dataset_preloaded()\n",
    "#dataset_train.load(\"full_\", device, length=32755)# 248348)\n",
    "sentences=True\n",
    "adding=\"\"\n",
    "if sentences:\n",
    "    adding=\"_sentences_\"\n",
    "\n",
    "dataset_real.load(\"dataset_real_75kv2\"+adding, device, name=\"val\", length=75000)\n",
    "dataset_abstract_train.load(\"dataset_abstract_train_60kv2\"+adding, device, name=\"train\", length=60000)\n",
    "dataset_abstract_test.load(\"dataset_abstract_test_15kv2\"+adding, device, name=\"val\", length=15000)\n",
    "dataset_abstract = ConcatDataset([dataset_abstract_train, dataset_abstract_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine abstract and real datasets\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, real=1):\n",
    "        self.existing_dataset = dataset\n",
    "        self.real = real\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.existing_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return  self.existing_dataset[index]+(self.real, )\n",
    "    \n",
    "dataset_real_extended = CustomDataset(dataset_real, real=1)\n",
    "dataset_abstract_extended = CustomDataset(dataset_abstract, real=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"combined\"\n",
    "if dataset_type == \"real\":\n",
    "    dataset = dataset_real\n",
    "elif dataset_type == \"abstract\":\n",
    "    dataset = dataset_abstract\n",
    "elif dataset_type == \"combined\":\n",
    "    dataset_combined = ConcatDataset([dataset_real_extended, dataset_abstract_extended])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "#Contrastive approach\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, dataset, relation_imbalaced):\n",
    "        self.new_dataset = []\n",
    "        for element in tqdm.tqdm(dataset):\n",
    "            samples = []\n",
    "            image, mc_answers, question, index_answer, answer_type, real_type = element\n",
    "\n",
    "            excluded_index = int(index_answer.cpu().item())\n",
    "            random_indexs_keep = np.random.choice([i for i in range(0, 18) if i != excluded_index], size=relation_imbalaced, replace=False)\n",
    "              \n",
    "            samples.append((image, mc_answers[excluded_index], question, 1, answer_type, real_type))\n",
    "            \n",
    "            for keep_index in random_indexs_keep:\n",
    "                samples.append((image, mc_answers[keep_index], question, 0, answer_type, real_type))\n",
    "            \n",
    "            self.new_dataset.extend(samples)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.new_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return  self.new_dataset[index]\n",
    "\n",
    "ratio_positive_negative = 4    \n",
    "#data_combined_contrastive = ContrastiveDataset(dataset_combined, relation_imbalaced = ratio_positive_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New with both Validation and Training Datasets from MSCOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  96000\n",
      "Val size:  24000\n",
      "Test size:  30000\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "\n",
    "train_size = int(len(dataset_combined)*0.8*0.8)\n",
    "val_size = int(len(dataset_combined)*0.8*0.2)\n",
    "test_size = int(len(dataset_combined))-val_size-train_size\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Val size: \", val_size)\n",
    "print(\"Test size: \", test_size)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset_combined, val_dataset_combined, test_dataset_combined = random_split(dataset_combined, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "train_dataloader_combined = DataLoader(train_dataset_combined, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_combined = DataLoader(test_dataset_combined, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader_combined = DataLoader(val_dataset_combined, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96000/96000 [01:02<00:00, 1523.81it/s]\n",
      "100%|██████████| 24000/24000 [00:20<00:00, 1186.74it/s]\n",
      "100%|██████████| 30000/30000 [01:16<00:00, 390.14it/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset_constrastive = ContrastiveDataset(train_dataset_combined, relation_imbalaced = ratio_positive_negative)\n",
    "val_dataset_constrastive = ContrastiveDataset(val_dataset_combined, relation_imbalaced = ratio_positive_negative)\n",
    "test_dataset_constrastive = ContrastiveDataset(test_dataset_combined, relation_imbalaced = ratio_positive_negative)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset_constrastive, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset_constrastive, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_constrastive, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model architectue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VQA_Model_Precalc_Zero, VQA_Model1_Precalc, VQA_Model_Precalc, VQA_Model4_Precalc, VQA_Model_Discr, VQA_Model_Discr_Siamese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class performanceAnalysis:\n",
    "    def __init__(self, dataset_type=\"combined\"):\n",
    "        self.combined = dataset_type==\"combined\"\n",
    "        self.dataset_type = dataset_type\n",
    "        self.counters = np.zeros((self.combined+1, 3)) \n",
    "        self.total = np.zeros((self.combined+1, 3))\n",
    "    def it(self, correct, answer_types, real_types=None):#both numpy arrays\n",
    "        if not self.combined:\n",
    "            for i in range(len(self.counters[0])):\n",
    "                self.counters[0][i]+=np.sum(np.logical_and(correct, answer_types==i))\n",
    "                self.total[0][i]+=np.sum(answer_types==i)\n",
    "        else:\n",
    "            is_real = (real_types==1).reshape(len(real_types), -1)\n",
    "            is_abstract = np.logical_not(is_real)\n",
    "            for i in range(len(self.counters[0])):\n",
    "                self.counters[0][i]+=np.sum(np.logical_and(np.logical_and(correct, answer_types==i), is_abstract))\n",
    "                self.total[0][i]+=np.sum(np.logical_and(answer_types==i, is_abstract))\n",
    "\n",
    "                self.counters[1][i]+=np.sum(np.logical_and(np.logical_and(correct, answer_types==i), is_real))\n",
    "                self.total[1][i]+=np.sum(np.logical_and(answer_types==i, is_real))\n",
    "\n",
    "    def get_accuracies(self):\n",
    "        if not self.combined:\n",
    "            dictionary = {self.dataset_type: {\"yes/no\": 100*self.counters[0][0]/self.total[0][0], \"number\": 100*self.counters[0][1]/self.total[0][1], \"other\": 100*self.counters[0][2]/self.total[0][2], \"total\": 100*sum(self.counters[0])/sum(self.total[0])}}\n",
    "        else:\n",
    "            dictionary_abstract = {\"yes/no\": 100*self.counters[0][0]/self.total[0][0], \"number\": 100*self.counters[0][1]/self.total[0][1], \"other\": 100*self.counters[0][2]/self.total[0][2], \"total\": 100*sum(self.counters[0])/sum(self.total[0])}\n",
    "            dictionary_real = {\"yes/no\": 100*self.counters[1][0]/self.total[1][0], \"number\": 100*self.counters[1][1]/self.total[1][1], \"other\": 100*self.counters[1][2]/self.total[1][2], \"total\": 100*sum(self.counters[1])/sum(self.total[1])}\n",
    "            dictionary = {\"abstract\": dictionary_abstract, \"real\": dictionary_real} \n",
    "        return dictionary\n",
    "    \n",
    "def printResults(dict_performance, performAnalysis):\n",
    "    info = \"\"\n",
    "    for i, element in enumerate(dict_performance):\n",
    "        info += (element+\" - \")\n",
    "        info += f\"total: {dict_performance[element]['total']:.2f}, yes/no: {dict_performance[element]['yes/no']:.2f}({performAnalysis.counters[i][0]}/{performAnalysis.total[i][0]}), number: {dict_performance[element]['number']:.2f}({performAnalysis.counters[i][1]}/{performAnalysis.total[i][1]}), other: {dict_performance[element]['other']:.2f}({performAnalysis.counters[i][2]}/{performAnalysis.total[i][2]})\"\n",
    "        if dataset_type==\"combined\" and i==0:\n",
    "            info += \"\\n\"\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def evaluate(model, dataloader, device, show_progress=False, score_each=False):\n",
    "    model.eval()\n",
    "    performAnalysis = performanceAnalysis(dataset_type)\n",
    "    correct = 0\n",
    "    sig = torch.nn.Sigmoid()\n",
    "    if show_progress:\n",
    "        pbar = tqdm.tqdm(dataloader)\n",
    "    else:\n",
    "        pbar = dataloader\n",
    "\n",
    "    for i, data in enumerate(pbar):\n",
    "        image = data[0].to(device)\n",
    "        answer_tokens = data[1].squeeze(0).to(device)\n",
    "        \n",
    "        correct_answer = torch.tensor([int(x) for x in data[3]]).to(device)\n",
    "        question_tokens = data[2].squeeze(1).to(device)\n",
    "\n",
    "        outputs = torch.zeros((answer_tokens.shape[0], answer_tokens.shape[1])).cuda()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for mc_answer_index in range(answer_tokens.shape[1]):\n",
    "                outputs[:, mc_answer_index] = sig(model(image, question_tokens, answer_tokens[:, mc_answer_index, :]).squeeze(1))\n",
    "\n",
    "            pred = outputs.argmax(dim=-1)\n",
    "            # get sum of correct answers\n",
    "            correct += (pred == correct_answer).sum().item()\n",
    "            if dataset_type != \"combined\":\n",
    "                performAnalysis.it((pred==correct_answer).cpu().numpy().reshape(len(pred), -1), data[4].cpu().numpy())\n",
    "            else:\n",
    "                performAnalysis.it((pred==correct_answer).cpu().numpy().reshape(len(pred), -1), data[4].cpu().numpy(), data[5].cpu().numpy())\n",
    "    dict_performance = performAnalysis.get_accuracies()\n",
    "    printResults(dict_performance, performAnalysis)\n",
    "    return dict_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSiamese(model, dataloader, device, show_progress=False, score_each=False):\n",
    "    model.eval()\n",
    "    performAnalysis = performanceAnalysis(dataset_type)\n",
    "\n",
    "    if show_progress:\n",
    "        pbar = tqdm.tqdm(dataloader)\n",
    "    else:\n",
    "        pbar = dataloader\n",
    "\n",
    "    for i, data in enumerate(pbar):\n",
    "        image = data[0].to(device)\n",
    "        target = data[1].to(device)\n",
    "        question_tokens = data[2].squeeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sig = torch.nn.Sigmoid()\n",
    "            output = sig(model(image, question_tokens, target).squeeze(1))\n",
    "            \n",
    "            label = data[3].to(device)\n",
    "\n",
    "            if dataset_type != \"combined\":\n",
    "                performAnalysis.it(((output > 0.5)*label).cpu().numpy().reshape(len(output), -1), data[4].cpu().numpy())\n",
    "            else:\n",
    "                performAnalysis.it(((output > 0.5)*label).cpu().numpy().reshape(len(output), -1), data[4].cpu().numpy(), data[5].cpu().numpy())\n",
    "    dict_performance = performAnalysis.get_accuracies()\n",
    "    printResults(dict_performance, performAnalysis)\n",
    "    return dict_performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contrastive learning\n",
    "def train_siamese(model, train_dataloader, val_dataloader, device, epochs=10, patience=3, precalculated=False, save_name=\"model\", loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([ratio_positive_negative]).cuda())):\n",
    "    print(device)\n",
    "    print(model.parameters())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=1e-4)\n",
    "\n",
    "    prev_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        inner_bar = tqdm.tqdm(train_dataloader, desc='Batch', colour='green')\n",
    "        loss_accum = 0\n",
    "        \n",
    "        model.train()\n",
    "        for data in inner_bar:\n",
    "            \n",
    "            image = data[0].to(device)\n",
    "            target = data[1].to(device)\n",
    "            question_tokens = data[2].squeeze(1).to(device)\n",
    "        \n",
    "            output = model(image, question_tokens, target)\n",
    "            \n",
    "            label = data[3].to(device)\n",
    "            loss = loss_fn(output.squeeze(1), label.float())\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0, error_if_nonfinite=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_accum += loss.item()\n",
    "        \n",
    "        dic = evaluateSiamese(model, val_dataloader, device)\n",
    "        acc = (dic[\"abstract\"][\"total\"]+dic[\"real\"][\"total\"])/2\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            # save model\n",
    "            model.save(save_name)\n",
    "        if acc < prev_acc:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        prev_acc = acc\n",
    "        print(f\"Epoch {epoch} train_loss: {loss_accum/len(train_dataloader)}, patience: {count}\")\n",
    "        if count == patience:\n",
    "            print(\"early stopping\")\n",
    "            break \n",
    "        inner_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "<generator object Module.parameters at 0x00000177A7379900>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   0%|\u001b[32m          \u001b[0m| 0/3750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:26<00:00, 142.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 16.60, yes/no: 19.12(4680.0/24475.0), number: 16.81(1411.0/8395.0), other: 14.25(3832.0/26900.0)\n",
      "real - total: 16.13, yes/no: 19.35(4365.0/22560.0), number: 15.95(1219.0/7645.0), other: 13.77(4133.0/30025.0)\n",
      "Epoch 0 train_loss: 0.7512066379944483, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.30, yes/no: 19.16(4689.0/24475.0), number: 17.53(1472.0/8395.0), other: 15.53(4178.0/26900.0)\n",
      "real - total: 16.62, yes/no: 19.14(4319.0/22560.0), number: 16.56(1266.0/7645.0), other: 14.74(4426.0/30025.0)\n",
      "Epoch 1 train_loss: 0.6690324909845988, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.90, yes/no: 19.55(4784.0/24475.0), number: 17.89(1502.0/8395.0), other: 16.40(4412.0/26900.0)\n",
      "real - total: 17.06, yes/no: 19.50(4400.0/22560.0), number: 17.20(1315.0/7645.0), other: 15.19(4562.0/30025.0)\n",
      "Epoch 2 train_loss: 0.638984479268392, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.83, yes/no: 19.33(4731.0/24475.0), number: 18.63(1564.0/8395.0), other: 16.22(4364.0/26900.0)\n",
      "real - total: 17.20, yes/no: 19.35(4365.0/22560.0), number: 18.02(1378.0/7645.0), other: 15.37(4615.0/30025.0)\n",
      "Epoch 3 train_loss: 0.6186385079542795, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.83, yes/no: 19.21(4702.0/24475.0), number: 18.27(1534.0/8395.0), other: 16.43(4420.0/26900.0)\n",
      "real - total: 16.85, yes/no: 19.30(4354.0/22560.0), number: 17.23(1317.0/7645.0), other: 14.91(4477.0/30025.0)\n",
      "Epoch 4 train_loss: 0.601695081226031, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.98, yes/no: 19.64(4807.0/24475.0), number: 18.31(1537.0/8395.0), other: 16.38(4405.0/26900.0)\n",
      "real - total: 16.91, yes/no: 19.55(4411.0/22560.0), number: 16.60(1269.0/7645.0), other: 14.99(4502.0/30025.0)\n",
      "Epoch 5 train_loss: 0.5879533090909322, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 18.34, yes/no: 19.73(4829.0/24475.0), number: 18.50(1553.0/8395.0), other: 17.02(4578.0/26900.0)\n",
      "real - total: 17.37, yes/no: 19.72(4449.0/22560.0), number: 17.31(1323.0/7645.0), other: 15.63(4692.0/30025.0)\n",
      "Epoch 6 train_loss: 0.5762225297451019, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.89, yes/no: 19.62(4801.0/24475.0), number: 18.34(1540.0/8395.0), other: 16.17(4349.0/26900.0)\n",
      "real - total: 16.43, yes/no: 19.32(4359.0/22560.0), number: 16.98(1298.0/7645.0), other: 14.11(4237.0/30025.0)\n",
      "Epoch 7 train_loss: 0.565629390390714, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 152.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 18.30, yes/no: 19.68(4817.0/24475.0), number: 18.78(1577.0/8395.0), other: 16.90(4546.0/26900.0)\n",
      "real - total: 17.30, yes/no: 19.55(4410.0/22560.0), number: 17.67(1351.0/7645.0), other: 15.52(4660.0/30025.0)\n",
      "Epoch 8 train_loss: 0.5560427691141764, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.56, yes/no: 19.51(4776.0/24475.0), number: 17.59(1477.0/8395.0), other: 15.78(4245.0/26900.0)\n",
      "real - total: 16.31, yes/no: 19.34(4364.0/22560.0), number: 16.70(1277.0/7645.0), other: 13.94(4184.0/30025.0)\n",
      "Epoch 9 train_loss: 0.5476855791012446, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 18.41, yes/no: 19.68(4816.0/24475.0), number: 18.12(1521.0/8395.0), other: 17.35(4667.0/26900.0)\n",
      "real - total: 17.63, yes/no: 19.56(4413.0/22560.0), number: 17.36(1327.0/7645.0), other: 16.25(4880.0/30025.0)\n",
      "Epoch 10 train_loss: 0.5391001469771067, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 18.35, yes/no: 19.68(4816.0/24475.0), number: 18.45(1549.0/8395.0), other: 17.12(4605.0/26900.0)\n",
      "real - total: 17.11, yes/no: 19.52(4403.0/22560.0), number: 17.25(1319.0/7645.0), other: 15.26(4582.0/30025.0)\n",
      "Epoch 11 train_loss: 0.5320007536967596, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.42, yes/no: 19.68(4817.0/24475.0), number: 17.26(1449.0/8395.0), other: 15.42(4147.0/26900.0)\n",
      "real - total: 16.22, yes/no: 19.43(4384.0/22560.0), number: 15.92(1217.0/7645.0), other: 13.88(4167.0/30025.0)\n",
      "Epoch 12 train_loss: 0.5244380723158518, patience: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 18.01, yes/no: 19.70(4821.0/24475.0), number: 17.89(1502.0/8395.0), other: 16.51(4440.0/26900.0)\n",
      "real - total: 16.77, yes/no: 19.51(4402.0/22560.0), number: 17.23(1317.0/7645.0), other: 14.58(4379.0/30025.0)\n",
      "Epoch 13 train_loss: 0.5177175584634145, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.89, yes/no: 19.69(4818.0/24475.0), number: 17.57(1475.0/8395.0), other: 16.35(4398.0/26900.0)\n",
      "real - total: 16.59, yes/no: 19.44(4385.0/22560.0), number: 16.39(1253.0/7645.0), other: 14.51(4356.0/30025.0)\n",
      "Epoch 14 train_loss: 0.5107682067950566, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.69, yes/no: 19.58(4793.0/24475.0), number: 17.97(1509.0/8395.0), other: 15.88(4271.0/26900.0)\n",
      "real - total: 16.65, yes/no: 19.41(4378.0/22560.0), number: 16.89(1291.0/7645.0), other: 14.52(4359.0/30025.0)\n",
      "Epoch 15 train_loss: 0.506152428038915, patience: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.72, yes/no: 19.53(4780.0/24475.0), number: 17.59(1477.0/8395.0), other: 16.12(4337.0/26900.0)\n",
      "real - total: 16.83, yes/no: 19.47(4392.0/22560.0), number: 16.76(1281.0/7645.0), other: 14.86(4462.0/30025.0)\n",
      "Epoch 16 train_loss: 0.499810741297404, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 151.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.72, yes/no: 19.55(4784.0/24475.0), number: 18.05(1515.0/8395.0), other: 15.96(4292.0/26900.0)\n",
      "real - total: 16.23, yes/no: 19.33(4361.0/22560.0), number: 16.10(1231.0/7645.0), other: 13.94(4184.0/30025.0)\n",
      "Epoch 17 train_loss: 0.4943122979402542, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 18.13, yes/no: 19.60(4797.0/24475.0), number: 18.37(1542.0/8395.0), other: 16.72(4498.0/26900.0)\n",
      "real - total: 16.97, yes/no: 19.33(4361.0/22560.0), number: 17.19(1314.0/7645.0), other: 15.13(4544.0/30025.0)\n",
      "Epoch 18 train_loss: 0.4887692974249522, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.73, yes/no: 19.55(4786.0/24475.0), number: 18.28(1535.0/8395.0), other: 15.89(4275.0/26900.0)\n",
      "real - total: 16.57, yes/no: 19.31(4356.0/22560.0), number: 16.86(1289.0/7645.0), other: 14.44(4336.0/30025.0)\n",
      "Epoch 19 train_loss: 0.48428134436209996, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.18, yes/no: 19.47(4765.0/24475.0), number: 17.55(1473.0/8395.0), other: 14.99(4033.0/26900.0)\n",
      "real - total: 15.96, yes/no: 19.10(4309.0/22560.0), number: 16.30(1246.0/7645.0), other: 13.52(4058.0/30025.0)\n",
      "Epoch 20 train_loss: 0.4796847851713498, patience: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 18.07, yes/no: 19.65(4810.0/24475.0), number: 17.32(1454.0/8395.0), other: 16.86(4535.0/26900.0)\n",
      "real - total: 16.74, yes/no: 19.45(4387.0/22560.0), number: 16.34(1249.0/7645.0), other: 14.81(4448.0/30025.0)\n",
      "Epoch 21 train_loss: 0.4736431425333023, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.17, yes/no: 19.35(4737.0/24475.0), number: 17.11(1436.0/8395.0), other: 15.20(4089.0/26900.0)\n",
      "real - total: 16.00, yes/no: 18.99(4284.0/22560.0), number: 16.09(1230.0/7645.0), other: 13.73(4121.0/30025.0)\n",
      "Epoch 22 train_loss: 0.46983724038998287, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.75, yes/no: 19.60(4796.0/24475.0), number: 17.77(1492.0/8395.0), other: 16.07(4324.0/26900.0)\n",
      "real - total: 16.25, yes/no: 19.31(4357.0/22560.0), number: 16.19(1238.0/7645.0), other: 13.96(4190.0/30025.0)\n",
      "Epoch 23 train_loss: 0.46603906942208606, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.62, yes/no: 19.51(4775.0/24475.0), number: 17.50(1469.0/8395.0), other: 15.94(4288.0/26900.0)\n",
      "real - total: 16.36, yes/no: 19.24(4341.0/22560.0), number: 16.39(1253.0/7645.0), other: 14.19(4261.0/30025.0)\n",
      "Epoch 24 train_loss: 0.46140211900870004, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.27, yes/no: 19.35(4736.0/24475.0), number: 17.50(1469.0/8395.0), other: 15.32(4120.0/26900.0)\n",
      "real - total: 16.00, yes/no: 19.10(4309.0/22560.0), number: 16.23(1241.0/7645.0), other: 13.62(4089.0/30025.0)\n",
      "Epoch 25 train_loss: 0.4580522923707962, patience: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.34, yes/no: 19.44(4757.0/24475.0), number: 16.83(1413.0/8395.0), other: 15.59(4193.0/26900.0)\n",
      "real - total: 15.96, yes/no: 19.08(4305.0/22560.0), number: 15.67(1198.0/7645.0), other: 13.69(4111.0/30025.0)\n",
      "Epoch 26 train_loss: 0.4537635591228803, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.63, yes/no: 19.58(4793.0/24475.0), number: 17.39(1460.0/8395.0), other: 15.92(4283.0/26900.0)\n",
      "real - total: 16.41, yes/no: 19.32(4359.0/22560.0), number: 16.43(1256.0/7645.0), other: 14.22(4269.0/30025.0)\n",
      "Epoch 27 train_loss: 0.45024709061781565, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.37, yes/no: 19.56(4788.0/24475.0), number: 16.46(1382.0/8395.0), other: 15.65(4211.0/26900.0)\n",
      "real - total: 16.32, yes/no: 19.30(4355.0/22560.0), number: 15.88(1214.0/7645.0), other: 14.19(4261.0/30025.0)\n",
      "Epoch 28 train_loss: 0.44735114761193595, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.85, yes/no: 19.50(4772.0/24475.0), number: 18.20(1528.0/8395.0), other: 16.23(4366.0/26900.0)\n",
      "real - total: 16.49, yes/no: 19.28(4350.0/22560.0), number: 16.76(1281.0/7645.0), other: 14.33(4303.0/30025.0)\n",
      "Epoch 29 train_loss: 0.44416854898134867, patience: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.64, yes/no: 19.61(4799.0/24475.0), number: 17.53(1472.0/8395.0), other: 15.89(4274.0/26900.0)\n",
      "real - total: 16.22, yes/no: 19.39(4375.0/22560.0), number: 16.06(1228.0/7645.0), other: 13.87(4165.0/30025.0)\n",
      "Epoch 30 train_loss: 0.44030265264908475, patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.52, yes/no: 19.35(4736.0/24475.0), number: 17.37(1458.0/8395.0), other: 15.90(4277.0/26900.0)\n",
      "real - total: 15.99, yes/no: 19.02(4291.0/22560.0), number: 15.63(1195.0/7645.0), other: 13.81(4146.0/30025.0)\n",
      "Epoch 31 train_loss: 0.4379663781881332, patience: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|\u001b[32m██████████\u001b[0m| 3750/3750 [00:24<00:00, 150.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 17.55, yes/no: 19.54(4783.0/24475.0), number: 17.44(1464.0/8395.0), other: 15.78(4245.0/26900.0)\n",
      "real - total: 15.85, yes/no: 19.24(4341.0/22560.0), number: 16.26(1243.0/7645.0), other: 13.20(3964.0/30025.0)\n",
      "Epoch 32 train_loss: 0.43359036070108414, patience: 3\n",
      "early stopping\n",
      "abstract - total: 17.44, yes/no: 19.50(5972.0/30630.0), number: 17.53(1975.0/11265.0), other: 15.55(5260.0/33820.0)\n",
      "real - total: 15.81, yes/no: 19.27(5388.0/27955.0), number: 15.77(1460.0/9260.0), other: 13.20(4893.0/37070.0)\n"
     ]
    }
   ],
   "source": [
    "trained_model = VQA_Model_Discr_Siamese(model, device)\n",
    "# freeze the clip model\n",
    "for param in trained_model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#evaluate(trained_model, test_dataloader, device, test_size, show_progress=True)\n",
    "improvement=adding+\"v2\"+\"_contrastive\"#+\"_dropout2e-1\"\n",
    "filename_save = trained_model.__class__.__name__+\"_\"+dataset_type+improvement\n",
    "train_siamese(trained_model, train_dataloader, val_dataloader, device, epochs=100, save_name=filename_save)\n",
    "# evaluate the model\n",
    "evaluateSiamese(trained_model, test_dataloader, device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 42.48, yes/no: 51.93(3181.0/6126.0), number: 36.71(827.0/2253.0), other: 35.85(2425.0/6764.0)\n",
      "real - total: 33.29, yes/no: 53.35(2983.0/5591.0), number: 19.71(365.0/1852.0), other: 21.55(1598.0/7414.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'abstract': {'yes/no': 51.9262161279791,\n",
       "  'number': 36.70661340434975,\n",
       "  'other': 35.85156712004731,\n",
       "  'total': 42.481674701182065},\n",
       " 'real': {'yes/no': 53.35360400643892,\n",
       "  'number': 19.70842332613391,\n",
       "  'other': 21.553817102778527,\n",
       "  'total': 33.2907047183146}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(trained_model, test_dataloader_combined, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "trained_model.save(\"trained_model_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:23<00:00, 19.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract - total: 55.24, yes/no: 62.99(3859.0/6126.0), number: 43.81(987.0/2253.0), other: 52.03(3519.0/6764.0)\n",
      "real - total: 47.01, yes/no: 59.26(3313.0/5591.0), number: 27.65(512.0/1852.0), other: 42.62(3160.0/7414.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model = VQA_Model_Precalc(model, device)\n",
    "dataset_type=\"combined\"\n",
    "improvement=adding+\"v2\"#\"_dropout2e-1\"\n",
    "filename_load = trained_model.__class__.__name__+\"_\"+dataset_type+improvement\n",
    "trained_model.load(filename_load)\n",
    "#print(trained_model)\n",
    "evaluate(trained_model, test_dataloader, device, show_progress=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
